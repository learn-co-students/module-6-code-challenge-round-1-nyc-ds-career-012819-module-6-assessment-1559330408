{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align:center\">Module 6 Assessment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your Mod 6 Assessment. You will be tested for your understanding on concepts and ability to programmatically solve problems that have been covered in class and in the curriculum. Topics in this assessment include graph theory, natural language processing, and neural networks. \n",
    "\n",
    "The goal here is to demonstrate your knowledge.  Showing that you know things is more important than getting the best model.\n",
    "\n",
    "Use any libraries you want to solve the problems in the assessment. \n",
    "\n",
    "You will have up to 90 minutes to complete this assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will attempt to classify text messages as \"SPAM\" or \"HAM\" using TF-IDF Vectorization. Once we successfully classify our texts we will examine our results to see which words are most important to each class of text messages. \n",
    "\n",
    "Complete the functions below and answer the question(s) at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df_messages = pd.read_csv('data/spam.csv', usecols=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string labels to 1 or 0 \n",
    "le = LabelEncoder()\n",
    "df_messages['target'] = le.fit_transform(df_messages['v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine or data\n",
    "df_messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate features and labels \n",
    "X = df_messages['v2']\n",
    "y = df_messages['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a list of stopwords \n",
    "stopwords_list = stopwords_list = stopwords.words('english') + list(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Let's create a function that takes in our various texts along with their respective labels and uses TF-IDF to vectorize the texts.  Recall that TF-IDF helps us \"vectorize\" text (turn text into numbers) so we can do \"math\" with it.  It is used to reflect how relevant a term is in a given document in a numerical way. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate tf-idf vectorization (use sklearn's TfidfVectorizer) for our data\n",
    "def tfidf(X, y,  stopwords_list): \n",
    "    '''\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify \n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    stopwords_list: list ojbect\n",
    "        List containing words and punctuation to remove. \n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "    y_train : array-like object\n",
    "        labels for training data\n",
    "    y_test : array-like object\n",
    "        labels for testing data\n",
    "    vectorizer : vectorizer object\n",
    "        fit TF-IDF vecotrizer object\n",
    "\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train, tf_idf_test, y_train, y_test, vecotorizer = tfidf(X, y, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Now that we have a set of vectorized training data we can use this data to train a classifier to learn how to classify a specific text based on the vectorized version of the text. Below we have initialized a simple Naive Bayes Classifier and Random Forest Classifier. Complete the function below which will accept a classifier object, a vectorized training set, vectorized test set, and list of training labels and return a list of predictions for our training set and a separate list of predictions for our test set.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that takes in a classifier and trains it on our tf-idf vectors and generates test and train predictiions\n",
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    '''\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    '''\n",
    "    #fit the classifier with our training data\n",
    "    \n",
    "    #predict the labels of our train data and store them in train_preds\n",
    "    \n",
    "    #predict the labels of our test data and store them in test_preds\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions for Naive Bayes Classifier\n",
    "nb_train_preds, nb_test_preds = classify_text(nb_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, nb_test_preds))\n",
    "print(accuracy_score(y_test, nb_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate predictions for Random Forest Classifier\n",
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see both classifiers do a pretty good job classifying texts as either \"SPAM\" or \"HAM\". Let's figure out which words are the most important to each class of texts! Recall that Inverse Document Frequency can help us determine which words are most important in an entire corpus or group of documents. \n",
    "\n",
    "<b>3) Create a function that calculates the IDF of each word in our collection of texts.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(class_, df, stopwords_list):\n",
    "    '''\n",
    "    Get ten words with lowest IDF values representing 10 most important\n",
    "    words for a defined class (spam or ham)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    class_ : str object\n",
    "        string defining class 'spam' or 'ham'\n",
    "    df : pandas DataFrame object\n",
    "        data frame containing texts and labels\n",
    "    stopwords_list: list object\n",
    "        List containing words and punctuation to remove. \n",
    "    --------\n",
    "    important_10 : pandas dataframe object\n",
    "        Dataframe containing 10 words and respective IDF values\n",
    "        representing the 10 most important words found in the texts\n",
    "        associated with the defined class\n",
    "    '''\n",
    "    #generate series containing all texts associated with the defined class\n",
    "    docs = 'code here'\n",
    "    \n",
    "    #initialize dictionary to count document frequency \n",
    "    # (number of documents that contain a certain word)\n",
    "    class_dict = {}\n",
    "    \n",
    "    #loop over each text and split each text into a list of its unique words \n",
    "    for doc in docs:\n",
    "        words = set(doc.split())\n",
    "        \n",
    "        #loop over each word and if it is not in the stopwords_list add the word \n",
    "        #to class_dict with a value of 1. if it is already in the dictionary\n",
    "        #increment it by 1\n",
    "        \n",
    "    #take our dictionary and calculate the \n",
    "    #IDF (number of docs / number of docs containing each word) \n",
    "    #for each word and return the 10 words with the lowest IDF \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_idf('spam', df_messages, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_idf('ham', df_messages, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain\n",
    "<b> 4) The word schools has the highest TF-IDF value in the second document of our test data. What does that tell us about the word school? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis Assessment\n",
    "\n",
    "For these next questions, you'll be using a graph dataset of facebook users and networkx. In the next cell, we're going to read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.read_edgelist('./data/0.edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1) Create a function `find_centrality` that returns a dictionary with the user with the highest betweenness centrality and the user with the highest degree centrality. It should return a dictionary that looks like:\n",
    "\n",
    "\n",
    "{'bc' : |user|, 'dc' : |user|}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centrality(graph):\n",
    "    \"\"\"\n",
    "    Calculates the most central nodes on a graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: networkx Graph object\n",
    "        Graph object to be analyzed\n",
    "    Returns\n",
    "    --------\n",
    "    centrality_dict : dict\n",
    "        A dictionary with the highest ranked user based off degree centrality and betweenness centrality \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) How does each of these people wield influence on the network? Imagine a message had to get to people from different communities. Who would be the best user to deliver the message to ensure that people from opposite communities receive the message?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) A marketing group is looking to target different communities with advertisements based off of their assumed mutual interests. Use the k_cliques_communities method to calculate the number of cliques formed with k users in a function `find_k_communities`. Calculate how many communities there are if the minimum size of a clique is 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_communities(graph,k):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: networkx Graph object\n",
    "        \n",
    "    k : int\n",
    "        k-number of connections required for a clique\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    num_communities: int\n",
    "        The number of communities present in the graph\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Assessment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning portion of this assessment is split into three main sections.  First, concepts from the introduction to deep learning are assessed by reconstructing the basic building blocks of a neural network.  Then, forward and back-propagation will be discussed in the “Multilayer Perceptron” section, as we build out a fully functioning neural network.\n",
    "\n",
    "Finally, you will be tuning and optimizing two neural networks trained on data generated with SKLearn — the first with regularization, and the second by modifying different aspects of the gradient descent process for deep learning.  You will receive credit for explaining your steps well even if the model does not improve much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_gaussian_quantiles, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential, regularizers\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>The Sigmoid Function</b></center>\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/perceptron.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) What are the inputs and outputs of a perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Given inputs and weights 1 through l and the sigmoid function (given above), write a function which computes the output y. Assume bias = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input_function):\n",
    "    \"\"\"\n",
    "    Transforms an input using the sigmoid function given above\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_function: function or numeric input to be transformed\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    output : float\n",
    "        result of the application of the sigmoid function \n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_output(x,w,b=0):\n",
    "    \"\"\"\n",
    "    Caluclates the perceptron output. Should use sigmoid as a helper function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        perceptron inputs\n",
    "    w : np.array\n",
    "        perceptron input weights\n",
    "    b : float\n",
    "        bias term\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    y : float\n",
    "        final output of the perceptron\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) What is the role of the sigmoid function here? How does what it does here relate to logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Name two other activation functions and write functions for them as done with the sigmoid in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_1(input_function):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_2(input_function):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Deeper_network_day2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward propagation\n",
    "\n",
    "$ Z^{[l]}= W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "$ A^{[l]}= g^{[l]} ( Z^{[l]})$\n",
    "\n",
    "##### Back-propagation\n",
    "$ dZ^{[l]}= dA ^{[l]} * g^{[l]'} (Z^{[l]})$\n",
    "\n",
    "$ dW^{[l]} = \\dfrac{1}{m} dZ^{[l]}* A^{[l-1]T}$\n",
    "\n",
    "$ db^{[l]} = \\dfrac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$\n",
    "\n",
    "$ dA^{[l-1]} = W^{[l]T}*dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Describe the process of forward propagation in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) How does what happens in forward-propagation change what happens in back-propagation? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7) Why is the chain rule important for backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8) You are training a neural network to pick out particular sounds in a dataset of audio files. Assume all preprocessing has already been done. If there are several sounds in each mp3 file, how would you modify your output layer to identify whether a particular sound occurs? How does your interpretation change assuming more than one sound can be in each file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and Optimization of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets are created using SKLearn, and should be improved. Although changing the number of nodes and layers may improve the models, focus on regularization in the first dataset, and gradient descent in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=450, noise=0.12)\n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'teal', 1:'orange'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    if key != 2:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: The following model is over-fit. Modify the following code to address the discrepancy between train and test accuracy.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test/split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9) Modify the code below to use L2 regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code goes in the cell below. Try running once without regularization first and look at what happens to train and test accuracy.\n",
    "\n",
    "Hint: use the activity_regularizer parameter in both of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#Instantiate Classifier\n",
    "classifier = Sequential()\n",
    "\n",
    "#Hidden Layer\n",
    "classifier.add(Dense(\n",
    "    32, \n",
    "    activation='relu', \n",
    "    input_dim=2,\n",
    "    kernel_initializer='random_normal',\n",
    "\n",
    "))\n",
    "\n",
    "#Hidden Layer\n",
    "classifier.add(Dense(\n",
    "    32,\n",
    "    activation='relu', \n",
    "    input_dim=2,\n",
    "    kernel_initializer='random_normal',\n",
    "\n",
    "))\n",
    "\n",
    "#Output Layer\n",
    "classifier.add(Dense(\n",
    "    1, \n",
    "    activation='sigmoid',\n",
    "    kernel_initializer='random_uniform',\n",
    "))\n",
    "\n",
    "classifier.compile(optimizer ='adam',loss=\"binary_crossentropy\",metrics =['accuracy'])\n",
    "\n",
    "classifier.fit(X_train, y_train, epochs=25, verbose=0, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look what happens to train and test accuracy as you modify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "#predict classes\n",
    "predicted_vals_train = classifier.predict_classes(X_train)\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_train,predicted_vals_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEST\n",
    "\n",
    "#predict classess\n",
    "predicted_vals_test = classifier.predict_classes(X_test)\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_test,predicted_vals_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10) Explain what you did and how it changed the train and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here // "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11) Explain what regularization does, and how it affects the final weights of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12) How does L1 regularization change a neural network's architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization with Gradient Descent\n",
    "\n",
    "A 3 dimensional dataset is generated using SKlearn and a poorly fit neural network is fit to it. Try improving the model using what's available through Keras, and then explain what you did in part 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/data.png' width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 3d data with complex error surface for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Construct dataset\n",
    "# Gaussian 1\n",
    "X1, y1 = make_gaussian_quantiles(cov=3.,\n",
    "                                 n_samples=10000, n_features=3,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X1 = pd.DataFrame(X1,columns=['x','y','z'])\n",
    "y1 = pd.Series(y1)\n",
    "\n",
    "# Gaussian 2\n",
    "X2, y2 = make_gaussian_quantiles(mean=(4, 4,2), cov=1,\n",
    "                                 n_samples=5000, n_features=3,\n",
    "                                 n_classes=2, random_state=2)\n",
    "X2 = pd.DataFrame(X2,columns=['x','y','z'])\n",
    "y2 = pd.Series(y2)\n",
    "# Combine the gaussians\n",
    "X1.shape\n",
    "X2.shape\n",
    "X = pd.DataFrame(np.concatenate((X1, X2)))\n",
    "y = pd.Series(np.concatenate((y1, - y2 + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13) Modify the code below to improve the starter model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use help(Dense) to see what parameters you can change. You should be able to explain how these parameters relate to gradient descent. Don't worry too much about overfitting in this example, just focus on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='zero', input_dim=3))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='zero'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='zero'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network, and specifying to measure accuracy at each step\n",
    "classifier.compile(optimizer ='sgd',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the neural network\n",
    "classifier.fit(X,y, batch_size=5, epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14) Explain why modifying the gradient descent process does anything and how it works. Include parameters you tried even if they did not improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// answer here //"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
